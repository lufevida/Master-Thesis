\chapter{PREVIOUS WORK}

% Formal languages lend precision and flexibility to music specification because they require that musical ideas be turned into abstract symbols and stipulated explicitly.

% Herein lies both the advantage and disadvantage of linguistic interaction with a computer music system.

% The advantage is that formalized and explicit instructions can yield a high degree of control.

% To create an imagined effect, composers need only specify it precisely.

% They can easily stipulate music that would be difficult or impossible to perform by human beings.

% In some cases, a linguistic specification is much more efficient than gestural input would be.

% This is the case when a single command applies to a massive group of events, or when a short list of commands replaces dozens of pointing and selecting gestures.

% The shell scripts of Unix operating systems are a typical example of command lists (Thompson and Ritchie 1974).

% These advantages turn into a disadvantage when simple things must be coded in the same detail and with the same syntactic overhead as complicated things.

% For example, with an alphanumeric language, envelope shapes that could be drawn on a screen in two seconds must be plotted out on paper by hand and transcribed into a list of numerical data to be typed by the composer.

% For many tasks, graphical editors and visual programming systems, in which the user selects and interconnects graphical objects, are more effective and easier to use than their textual counterparts (see chapter 16).

% Some languages are interactive; one can type individual statements and each of them is interpreted in turn.

% This can occur in a concert situation, but the slow information rate of typing — not to mention the mundane stage presence of a typist — precludes this approach in fast­paced real­time music­making.

% Gestural control through a musical input device is more efficient and natural.

% Hence, languages for music, although important, do not answer all musical needs.

% In the ideal, music languages should be available alongside other kinds of musical interaction tools.

% \cite[785-786]{Roads1995}.

% Textual languages are a precise and flexible means of controlling a computer music system.

% In synthesis, score entry, and composition, they can liberate the potential of a music system from the closed world of canned software and preset hardware.

% In the mid­-1980's it looked as if the Music N synthesis language dynasty might languish, due to the spread of inexpensive synthesizers.

% In MIDI systems, however, the use of a score language is less common, since most music can be entered by other means (such as a music keyboard, notation program, or scanner).

% For musicological applications that involve score analysis, however, a text­-based score representation may be optimum.

% New textual languages for procedural composition continue to be developed, but there is a strong parallel trend toward interactive programs with graphical interfaces.

% Instead of typing text, one patches together icons, draws envelopes, and fills in templates.

% The textual language representation supporting the graphics is hidden from the user.

% Early programming languages for music tended to favor machine efficiency over ease of use.

% The present trend in programming has shifted from squeezing the last drop of efficiency out of a system to helping the user manage the complexity of layer upon layer of software and hardware.

% The most common solution to this problem is object­-oriented programming (see chapter 2), and compositional applications are no exception to this trend (Pope 1991b).

% \cite[817]{Roads1995}.

Arguably the first notable attempt to design a programming language with an explicit intent of processing sounds and making music was that of \emph{Music I}, created in 1957 by Max Mathews. The language was indented to run on an IBM 704 computer, located at the IBM headquarters in New York City. The programs created there were recorded on digital magnetic tape, then converted to analog at Bell Labs, where Mathews spent most of his career as an electrical engineer. \emph{Music I} was capable of generating a single waveform, namely a triangle, as well as assigning duration, pitch, amplitude, and the same value for decay and release time. \emph{Music II} followed a year later, taking advantage of the much more efficient IBM 7094 to produce up to four independent voices, chosen from 16 waveforms. With \emph{Music III}, Mathews introduced in 1960 the \emph{unit generator} concept, which consisted of small building blocks of software that allowed composers to make use of the language with a lot less effort and required background. In 1963, \emph{Music IV} introduced the use of macros, which had just been invented, although the programming was still done in assembly language, hence all implementations of the language remained machine-dependent. With the increasing popularity of \emph{Fortran}, Mathews designed \emph{Music V} with the intent of making it machine-independent, at least in part, since the unit generators' inner loops were still programmed in machine language. The reason for that is the burden these loops imposed on the computer \cite[15-17]{Roads1980}.

\section{Software Synthesis Languages}

Since Mathews' early work, much progress has been made, and a myriad of new programming languages that support sound processing, as well as domain-specific languages whose sole purpose is to process sounds or musical events, have surfaced. \cite{Roads1995} provides a classification of these languages according to the specific aspect of sound processing they perform best. The first broad category described is that of \emph{software synthesis languages}, which compute samples in non-real-time, and are implemented by use of a text editor with a general purpose computer. The \emph{Music N} family of languages consists of software synthesis languages. A characteristic common to all software synthesis languages is that of a toolkit approach to sound synthesis, whereby using the toolkit is straightforward, however customizing it to fulfill particular needs often requires knowledge of the programming language in which the toolkit was implemented. This approach provides great flexibility, but at the expense of a much steeper learning curve. Another aspect of software synthesis languages is that they can support an arbitrary number of voices, and the time complexity of the algorithms used only influences the processing time, not the ability to process sounds at all, as can be seen in real-time implementations. As a result of being non-real-time, software synthesis languages usually lack controls that are gestural in nature. Yet, software synthesis languages are capable of processing sounds with a very fine numerical precision, although this usually translates to more detailed, hence verbose code. Software synthesis languages, or non-real-time features of a more general-purpose language, are sometimes required to realize specific musical ideas and sound-processing applications that are impossible to realize in real time \cite[783-787]{Roads1995}.

Within the category of software synthesis languages, some languages can be further classified into \emph{unit generator languages}. This is exactly the paradigm originally introduced by \emph{Music III}. In them, there is usually a separation between an orchestra section and a score section, often given by different files, and possibly by different sub-languages. A unit generator is more often than not a built-in feature of the language. Unit generators can generate or transform buffers of audio data, as well as deal with how the language interacts with the hardware, that is, provide sound input, output, or print statements to the console. Even though one can usually define unit generators in terms of the language itself, the common practice is to define them as part of the language implementation. Another characteristic of unit generators is that they are designed to take, as input arguments, the outputs of other unit generators, thus creating a signal flow. This is implemented by keeping data arrays in memory which are shared by reference between more different UG procedures. The score sub-language usually consists of a series of statements that call the routines defined by the orchestra sub-language in sequential order, often without making use of control statements. Another important aspect of the score sub-language is that it defines function lookup tables, which are mainly used to generate waveforms and envelopes. When \emph{Music N} languages became machine-independent, function generating routines remained machine-specific for a period of time, due to performance concerns. On the other hand, the orchestra sub-language is where the signal processing routines are defined. These routines are usually called instruments, and basically consist of new scopes of code where built-in functions (unit generators) are dove-tailed, ultimately to a unit generator that outputs sound or a sound file \cite[787-794]{Roads1995}.

The compilation process in \emph{Music N} languages consists usually of three passes. The first pass is a preprocessor, which optimizes the score that will be fed into the subsequent passes. The second pass simply sorts all function and instrument statements into chronological order. The third pass then executes each statement in order, either by filling up tables, or by calling the instrument routines defined in the orchestra. The third pass used to be the performance bottleneck in these language implementations, and during the transition between assembly and \emhp{Fortran} implementations, these were the parts that remained machine-specific. Initially, the output of the third pass consisted of a sound file, but eventually this part of the compilation process was adapted to generate real-time output. At that point, defining specific times for computing function tables became somewhat irrelevant. In some software synthesis languages, the compiler offered hooks in the first two passes so that users could define their own sound-processing subroutines. These extensions to the language were given in an altogether different language. With \emph{Common Lisp Music}, for example, one could define the data structures and control flow in terms of \emph{Lisp} itself, whereas \emph{MUS10} supported the same features by accepting \emph{Algol} code. In \emph{Csound}, one can still define control statements in the score using \emph{Python}. Until \emph{Music IV} and its derivatives, compilation was sample-oriented. As an optimization, \emph{Music V} introduced the idea of computing samples in blocks, where audio samples maintained their time resolution, but control statements could be computed only once per block. Of course, if the block size is one, than control values are computed for each sample, as in the sample-oriented paradigm. Instead of defining a block size, however, one defines a control rate, which is simply the sampling rate times the reciprocal of the block size. Hence a control rate that equals the sampling rate would indicate a block size of one. With \emph{Cmusic}, for instance, the block size is specified directly, a notion that is consistent with the current practice of specifying a vector size in real-time implementations. The idea of determining events in the language that could be computed at different rates required some sort of type declaration. In \emph{Csound}, these are given by naming conventions: variables whose names start with the character `a' are audio-rate variables, `k' means control rate, and `i'-variables values are computed only once per statement. \emph{Csound} also utilizes naming conventions to determine scopes, with the character `g' indicating whether a variable is global \cite[799-802]{Roads1995}.

\begin{table}[htbp]
	\caption{Music Programming Languages of the Sixties and Seventies.}
	\centering
	\vspace{12pt}
	\begin{tabular}{ *{4}{l} }
		\hline
		Name & Year & References & Classification \\
		\hline
		Music III & 1960 & \cite{Roads1980,Manning2004} & Software Synthesis \\
		Music IV & 1963 & \cite{Roads1980,Manning2004} & Software Synthesis \\
		MUSICOMP & 1963 & \cite{Baker1964} & Procedural Composition \\
		Music IVB & 1965 & \cite{Truax1976} & Software Synthesis \\
		Music 4F & 1966 & \cite{Howe1966} & Software Synthesis \\
		Music V & 1966 & \cite{Beauchamp1979,Pope1993} & Software Synthesis \\
		MUS10 & 1966 & \cite{McNabb1981} & Software Synthesis \\
		MUSIGOL & 1966 & \cite{MacInnis1968} & Software Synthesis \\
		Music 4BF & 1967 & \cite{Beauchamp1979,Gerrard1989} & Software Synthesis \\
		Music 360 & 1969 & \cite{Vercoe1999} & Software Synthesis \\
		Music 7 & 1969 & \cite[789-790]{Roads1995} & Software Synthesis \\
		TEMPO & 1970 & \cite{Clough1970} & Software Synthesis \\
		MUSTRAN & 1972 & \cite{Byrd1977} & Score Input \\
		SCORE & 1972 & \cite{Taube1991} & Score Input, \\
		&&& Procedural Composition \\
		Music 11 & 1973 & \cite{Vercoe1999} & Software Synthesis \\
		DARMS & 1975 & \cite{Brinkman1986} & Score Input \\
		Scriptu & 1977 & \cite{Brown1978} & Score Input \\
		MPL & 1977 & \cite{Roads1978} & Procedural Composition \\
		Musica & 1978 & \cite[811]{Roads1995} & Score Input \\
		MUSCMP & 1978 & \cite{McNabb1981} & Software Synthesis \\
		SYN4B & 1978 & \cite{Rolnick1978} & Real-Time Control \\
		4PLAY & 1978 & \cite[807-808]{Roads1995} & Real-Time Control \\
		PLAY & 1978 & \cite{Chadabe1978} & Procedural Composition \\
		Tree and Cotree & 1978 & \cite{Roads1979} & Procedural Composition \\
		4C & 1979 & \cite{Moorer1979} & Real-Time Control \\
		Musbox & 1979 & \cite{Loy1981} & Real-Time Control \\
		PILE & 1979 & \cite{Berg1979} & Procedural Composition \\
		SAWDUST & 1979 & \cite{Hamman2002} & Procedural Composition \\
		\hline
	\end{tabular}
\end{table}

\section{Real­-Time Synthesis Control Languages}

Some of the very first notable attempts to control synthesis hardware in real-time were made at the \emph{Institut de Recherche et Coordination Acoustique/Musique} in the late seventies. Many of these early attempts made use of programming languages to drive the sound synthesis being carried out by a dedicated DSP. At first, most implementations relied on the concept of a \emph{fixed-function} hardware, which required significantly simpler software implementations, as the latter served mostly to control a circuit that had an immutable design and function. An example of such fixed-function implementations would be an early frequency-modulation synthesizer, which contained a dedicated DSP for FM-synthesis, and whose software implementation would only go as far as controlling the parameters thereof. Often, the software would control a chain of interconnected dedicated DSP's, which would in turn produce envelopes, filters, and oscillators. The idea of controlling parameters through software, while delegating all signal processing to hardware, soon expanded beyond the control of synthesis parameters, and into the sequencing of musical events, like in the \emph{Synclavier} by New England Digital. Gradually, these commercial products began to offer the possibility of changing how exactly this components were interconnected, a concept that is called \emph{variable-function} DSP hardware. Interconnecting these components through software became commonly called \emph{patching}, as an analogy to analog synthesizers. The idea of patching brought more flexibility, but imposed a steeper learning curve to musicians. Eventually, these dedicated DSP's were substituted by general-purpose computers, wherein the entire chain of signal processing would be accomplished via software \cite[802-804]{Roads1995}.

Commonly in a fixed-function implementation there is some sort of front panel with a small LCD, along with buttons and knobs to manage user input. In the case of a keyboard instrument, there is naturally a keyboard to manage this interaction, as well. The purpose of the embedded software is then to communicate user input to an embedded system which contains a microprocessor and does the actual audio signal processing, memory management, and audio input/output. All software is installed in some read-only memory, including the operating system. With the creation of the \emph{Musical Instrument Digital Interface} standard in 1983, which was promptly absorbed my most commercial brands, the issue of controlling sound synthesis hardware transcended the interaction with keys, buttons, and sliders, and became a matter of software programming, as one could easily communicate with dedicated hardware, by means of a serial interface, MIDI messages containing discrete note data, continuous controller messages, discrete program change messages, as well as system­-exclusive messages. As a trend, many MIDI libraries were written at the time for general-purpose programming languages such as \emph{APL}, \emph{Basic}, \emph{C}, \emph{Pascal}, \emph{Hypertalk}, \emph{Forth}, and \emph{Lisp}. In addition, most descendants of the \emph{Music N} family of languages began to also support MIDI messages as a way to control dedicated hardware \cite[804-805]{Roads1995}.

The implementation of a software application to control variable-function DSP hardware is no mundane task, as it requires knowledge of digital signal processing, in addition to programming in a relatively low level language. Dealing with issues of performance, memory management, let alone the mathematics required to process buffers of audio samples, often imposes an unsurmountable burden to musicians. Many solutions were invented in order to work around these difficulties, including the use of graphic elements and controllers, but ultimately it was the concept of a unit generator, borrowed from software synthesis languages, that most influenced the creation of higher-level abstractions that were more suitable for musicians. This is notably the case of the \emph{4CED} language, which was developed at IRCAM in 1980, and owed greatly to \emph{Music IV} and \emph{Music V}. The resemblance extended as far as to comprise a separate orchestra sub-language for patching unit generators, a score sub-language, and a third command sub-language for controlling effects in real-time, as well as to link both orchestra and score to external input devices such as buttons and potentiometers. The hardware these languages drove was IRCAM's \emph{4C} synthesizer. The result of nearly a decade of research at IRCAM culminated in \emph{Max}, a visual programming language that remains to this day one of the most important real-time tools for musicians. \emph{Max}, which will later be discussed in more detail, eventually transcended its hardware DSP origins, incorporating all sound-generating routines. But that was not until the 2000's, ten years after it became a commercial software application, independent of IRCAM \cite[805-806]{Roads1995}. \emph{Music 1000} is a descendant of the \emph{Music N} family of languages that was designed to drive the Digital Music Systems \emph{DMX­1000} signal processing computer, in which the unit-generator concept can be clearly observed. Listing \ref{alg:music1000} exemplifies how a simple instrument is defined in \emph{Music 1000} \cite[809]{Roads1995}.

\begin{lstlisting}[emph={fnctn,instr,kscale,oscil,out,endin,fourier,normal},emphstyle={\textbf},caption={\emph{Music 1000} algorithm that produces a sine wave.},label={alg:music1000}]
fnctn func1, 512, fourier, normal, 1, 1000
instr 1
	kscale amp, knob1, 0, 10000
	kscale freq, knob2, 20, 2000
	oscil x8, #func1, amp, freq
	out x8
endin
\end{lstlisting}

\begin{enumerate}
	\item A \lstinline{fnctn} declaration assigns to variable \il{func1} an array of 512 samples using a \il{fourier} series of exactly 1 (harmonically-related) sine, whose (trivial) sum is \il{normal}-ized. The amplitude of 1000 is then meaningless, but a required argument. In fact, \il{func1} takes a variable number of arguments where, for each harmonic partial, the user specifies a relative amplitude.
	\item The block that follows defines instrument 1, in which the unit generator \il{oscil} takes as arguments the output of three other unit generators, which are respectively the wavetable previously computed, as well as amplitude and frequency parameters, whose values are in turn captured by two knobs attached to the machine. The knobs produce values between 0 and 1, and the subsequent arguments to \lstinline{kscale} are scaling parameters.
	\addtocounter{enumi}{3}
	\item \il{out} is a unit generator that connects the output of \il{oscil} to the digital-to-analog converter.
\end{enumerate}

\begin{table}[htbp]
	\caption{Music Programming Languages of the Early Eighties.}
	\centering
	\vspace{12pt}
	\begin{tabular}{ *{4}{l} }
		\hline
		Name & Year & References & Classification \\
		\hline
		4CED & 1980 & \cite{Abbott1981} & Real-Time Control \\
		Music 1000 & 1980 & \cite{Wallraff1979} & Real-Time Control \\
		Cmusic & 1980 & \cite{Moore1982,Pope1993} & Software Synthesis \\
		Yet Another Music Input Language & 1980 & \cite[811]{Roads1995} & Score Input \\
		SCRIPT & 1981 & \cite{CMJ1984} & Score Input \\
		4X & 1981 & \cite{Lippe1996} & Real-Time Control \\
		GGDL & 1981 & \cite{Holtzman1981} & Procedural Composition \\
		FMX & 1982 & \cite[807-808]{Roads1995} & Real-Time Control \\
		Music 400 & 1982 & \cite[807-808]{Roads1995} & Real-Time Control \\
		Cleo & 1983 & \cite[807-808]{Roads1995} & Real-Time Control \\
		Music 320 & 1983 & \cite[807-808]{Roads1995} & Real-Time Control \\
		Pla & 1983 & \cite{Schottstaedt1983} & Procedural Composition \\
		Music 500 & 1984 & \cite{Puckette2002} & Real-Time Control \\
		Cmix & 1984 & \cite{Pope1993,Collins2017} & Software Synthesis \\
		Flavors Band & 1984 & \cite{Fry1984} & Procedural Composition \\
		FORMES & 1984 & \cite{Rodet1984} & Procedural Composition \\
		ASHTON & 1985 & \cite{Dannenberg1993} & Score Input \\
		Music 4C & 1985 & \cite{Gerrard1989,Pope1993} & Software Synthesis \\
		Personal Composer Lisp & 1985 & \cite[815-817]{Roads1995} & Procedural Composition \\
		4XY & 1986 & \cite{Lippe1996} & Real-Time Control \\
		Arctic & 1986 & \cite{Dannenberg1986} & Procedural Composition \\
		Hyperscore & 1986 & \cite{Pope1989} & Procedural Composition \\
		Player & 1986 & \cite{Loy2002} & Procedural Composition \\
		Formula & 1986 & \cite{Anderson1989} & Procedural Composition \\
		MIDI­-LISP Toolkit & 1986 & \cite{Logemann1987} & Procedural Composition \\
		Csound & 1986 & \cite{Pope1993,Vercoe1999} & Software Synthesis, \\
		&&& Real-Time Control \\
		\hline
	\end{tabular}
\end{table}

\section{Music Composition Languages}

Between the 1960's and the 1990's, many programming languages were devised to aid music composition. As a noticeable trend, one can define two categories among those languages, namely those that are \emph{score input languages}, and those that are \emph{procedural composition languages}. The main difference between the two categories is that, in the former, some representation of a musical composition is already at hand, hence score input languages provide a way to encode that information. This representation could be a score, a MIDI note list, or even some graphical representation of music. In the latter category, the language provides, or helps define procedures that are used to generate musical material, a practice that is often called \emph{algorithmic music composition}. One outstanding characteristic of score input languages is how verbose and complex they can become, depending on the musical material they are trying to represent. This difficulty influenced the devising of many alternatives to textual programming languages, such as the use of scanners in the late 1990's by Neuratron's \emph{PhotoScore}, a technique which was predicted by composer Milton Babbitt in as early as 1965. Before the advent of MIDI, however, programming languages were indeed the user interface technology of choice, or lack thereof, to design applications meant for analyzing, synthesizing, and printing musical scores. With the widespread adoption of the MIDI standard in the mid-1980's, where one could input note events by performing on a MIDI instrument, combined with the advancements in graphical user interfaces of the mid-1990's, the creation and maintenance of score input languages faced a huge decline. What is even worse, the paradigm of a musical score is itself inadequate for computer music synthesis, in that a score is more often than not a very incomplete representation of a musical piece, often omitting a great deal of information. It is usually the job of a musical performer to provide that missing information. In this sense, \emph{procedural languages} are much better suited for computer performance, but that comes at the cost of replacing the score paradigm altogether \cite[811-813]{Roads1995}.

\emph{Musica} was developed at the \emph{Centro di Sonologia Computazionale} in Padua, Italy, and is particularly interesting in its interpreter compiles programs into \emph{Music V} note statements. Listing \ref{alg:padua} provides a simple example of \emph{Musica} code. In it, all numbers indicate note duration, that is, 4 is a quarter-note, 8 is an eighth-note, and 2 is half-note, with dots indicating dotted durations. The letters indicate pitch, and the apostrophe indicates octave, such that '$A = 440$Hz, and ''$A = 880$Hz. Finally, slashes indicate new measures \cite[812]{Roads1995}.

\begin{lstlisting}[caption={\emph{Musica} algorithm that creates a simple melodic line.},label={alg:padua}]
4'AGAG / 4.A8G2E / 4DDFD / 2ED
\end{lstlisting}

\begin{figure}[h]
	\caption{Typesetting music with \emph{MusiXTex}.}
	\label{doremi}
	\centering
	\begin{music}
		\generalmeter{\meterfrac44}
		\startextract
		\Notes \qu{h g h g} \en \bar
		\Notes \qup{h} \cu{g} \hu{e} \en \bar
		\Notes \qu{d d f d} \en \bar
		\Notes \hu{e d} \en
		\endextract
	\end{music}
\end{figure}

In 2018, a few score input languages remain, despite the vast predominance of graphical user interfaces as a means to input notes to a score. \emph{MusiXTex} is a surviving example that compiles to \LaTeX, which in turn compiles to PDF documents. It was created in 1991 by Daniel Taupin. The language has such unwieldy syntax, that often a preprocessor is required for more complex scores. One famous such processor is \emph{PMX}, a \emph{Fortran} tool written by Don Simons in the late 1990's. Another was \emph{MPP}, which stands for MusiXTex Preprocessor, created by Han-Wen Nienhuys and Jan Nieuwenhuizen in 1996, and which eventually became \emph{LilyPond}, arguably the most complete surviving score input language today. \emph{LilyPond} has a much simpler syntax than that of \emph{MusiXTex}, however not nearly as simple as \emph{ABC} music notation, a language that much resembles \emph{Musica} and which is traditionally used in music education contexts. A package written by Guido Gonzato that can produce simple scores in \emph{ABC} notation is available in \LaTeX. Its simplicity comes, however, at the expense of incompleteness. Finally, it is worthwhile to mention a music-notation specific standard that has emerged in the mid-2000's, namely the \emph{MusicXML} standard. Heavily influenced by the industry, it was initially meant as an object model to translate scores between commercial applications where the score input method was primarily graphical, and whose underlying implementation was naturally object-oriented. \emph{MusicXML} is extremely verbose, and borderline human-readable. It is, however, very complete, to the point of dictating what features an object-oriented implementation should comprise in order to be aligned with the industry standards. In recent years, many rumors have surfaced to make \emph{MusicXML} an Internet standard, such as that of \emph{Scalable Vector Graphics}, however nothing concrete has been established. Listing \ref{alg:latex} shows a \emph{MusiXTex} example of the very same musical material given in Listing \ref{alg:padua}. One can immediately notice the difference in implementation by the sheer amount of code required to express basically the same symbols.

\begin{lstlisting}[caption={\emph{MusiXTex} algorithm whose output is shown in Fig.~\ref{doremi}.},label={alg:latex}]
\begin{music}
	\generalmeter{\meterfrac44}
	\startextract
	\Notes \qu{h g h g} \en \bar
	\Notes \qup{h} \cu{g} \hu{e} \en \bar
	\Notes \qu{d d f d} \en \bar
	\Notes \hu{e d} \en
	\endextract
\end{music}
\end{lstlisting}

\begin{enumerate}
	\addtocounter{enumi}{3}
	\item The \il{\qu} command means a quarter-note with a stem pointing upward, whereas the \il{\Notes} command actually means how notes should be spaced. The more capital letters, the more spacing between the notes, that is, \lstinline{\NOTes} is more spaced out than \lstinline{\NOtes}.
	\item In addition to supporting the same apostrophes as \emph{Musica} for defining octaves, \emph{MusiXTex} also supports other letters, as well as capitalizations thereof. Here $h = 440$Hz, whereas $a = 220$Hz.
\end{enumerate}

One of the greatest contributions of \emph{procedural composition languages} to the field of music composition is arguably the concept of algorithmic composition, in particular when the realization of the musical algorithm is not restricted to human performers. In such circumstances, the composer is capable of exploring the full extent of musical ideas a computer can reproduce. Naturally, the composer must often trade off the ability to represent those ideas via a score, in which case the algorithm itself becomes the representation. If, on one hand, reading music from an algorithm is somewhat unfamiliar to most musicians, the representation is nonetheless formal, concise, and consistent. Furthermore, it lends itself to be analyzable by a much larger apparatus of analytical techniques and visualization tools, hence is equally beneficial a representation to music theorists. A machine is capable of representing all sorts of timbres, metrics, and tunings that humans cannot, but it needs to be told exactly what to do. Unlike a human performer, who interprets the composer's intents, a purely electro-acoustic algorithmic composition must address a human audience without relying on a middle-man. Hence the programming language of choice becomes an invaluable tool for the composer. In addition, another important aspect of algorithmic composition is how it is capable of transforming the decision-making process of a composer. Instead of making firm choices at the onset of a musical idea, a composer can \emph{prototype} many possible outcomes of that idea before deciding. One example is assigning random numbers to certain parameters, thus postponing decision making until more structure has been added to the composition. In fact, this postponing may be final, hence an algorithmic composition may be situated within a whole spectrum of determinism. A fully stochastic piece fixes no parameter, as opposed to a fully deterministic composition. Some of the notable techniques of electro-acoustic music composition also include spatialization, where the emission of sounds through speakers positioned at specific spatial locations constitutes a major musical dimension in a composition; spectralism, where the spectral content of sounds are manipulated by an algorithm; processing sound sources in real time, very often capturing a live performance on stage; and sonification of data not originally conceived as sound \cite[813]{Roads1995}.

\begin{table}[htbp]
	\caption{Music Programming Languages of the Late Eighties and Early Nineties.}
	\centering
	\vspace{12pt}
	\begin{tabular}{ *{4}{l} }
		\hline
		Name & Year & References & Classification \\
		\hline
		HMSL & 1987 & \cite{Polansky1990} & Procedural Composition \\
		Esquisse & 1988 & \cite{Rahn1990} & Procedural Composition \\
		LOCO & 1988 & \cite{Desain1988} & Procedural Composition \\
		Standard Music Description Language & 1989 & \cite[811]{Roads1995} & Score Input \\
		Music 56000 & 1989 & \cite{Pinkston1989} & Real-Time Control \\
		NeXT Music and Sound Kits & 1989 & \cite{Jaffe1989} & Real-Time Control \\
		PatchWork & 1989 & \cite{Assayag1999} & Procedural Composition \\
		Moxc & 1989 & \cite{Boulanger1990} & Procedural Composition \\
		Sound and Music Kits & 1989 & \cite{Jaffe1989} & Procedural Composition \\
		Canon & 1989 & \cite{Dannenberg1989} & Procedural Composition \\
		COMPOSE & 1989 & \cite{Ames1992} & Procedural Composition \\
		Notepro & 1990 & \cite{Alphonce1989} & Score Input \\
		Digital Signal Patcher & 1990 & \cite{Bianchini1992} & Real-Time Control \\
		Lisp Kernel & 1990 & \cite{Rahn1990} & Procedural Composition \\
		Keynote & 1990 & \cite[815-817]{Roads1995} & Procedural Composition \\
		MUSIC30 & 1991 & \cite{Dashow1996} & Real-Time Control \\
		IRCAM Max & 1991 & \cite{Puckette2002} & Real-Time Control \\
		Common Music & 1991 & \cite{Taube1991} & Software Synthesis, \\
		&&& Procedural Composition \\
		MODE & 1991 & \cite{Pope1995} & Procedural Composition \\
		IRIS Edit20 & 1992 & \cite{Prestigiacomo1997} & Real-Time Control \\
		Unison & 1992 & \cite[807-808]{Roads1995} & Real-Time Control \\
		Cadenza & 1993 & \cite{Field1993} & Score Input \\
		Symbolic Composer & 1993 & \cite{Sica1994} & Procedural Composition \\
		\hline
	\end{tabular}
\end{table}

\section{Extensibility and Libraries}

Many domain-specific languages that deal with sound synthesis, processing, and music composition are \emph{extensible} in the sense that they provide a hook for code written in the implementation language to be executed in the context of the DSL. This feature can render a DLS a lot more flexible, at the expense of annulling the very purpose of the DSL, which can be a good trade-off if the latter's implementation is incomplete. An early example would be \emph{Music V}, which could accept user-written subroutines in \emph{Fortran}. \emph{Music 4C} had its instruments written in \emph{C}, and \emph{Cscore} was a \emph{C}-embedding of \emph{Cmusic}. Other examples are \emph{MPL}, which could accept routines written in \emph{APL}, and \emph{Pla}, whose first version was embedded in \emph{Sail}, and whose second version was embedded in \emph{Lisp}. In the particular case of \emph{Lisp}, embeddings include \emph{MIDI-LISP}, \emph{FORMES}, \emph{Esquisse}, \emph{Lisp Kernel}, \emph{Common Music}, \emph{Symbolic Composer}, \emph{Flavors Band}, and \emph{Canon}. \emph{Music Kit} was embedded in the object-oriented \emph{Objective-C} \cite[814]{Roads1995}.

Besides domain-specific languages, a variety of libraries exist for general-purpose programming languages that also deal with aspects of sound synthesis, processing, and music composition. In functional languages, these libraries may carry such syntactical weight, with so many specifically-defined symbols and overloaded operators, that they do in fact resemble more of a DSL than a library, even though such terming would not be technically correct. \cite{Hudak2018} provides a \emph{Haskell} library that covers many aspects of music production, from algorithmic composition to spectral analysis in a functional-oriented manner. A similar project is that of \emph{Common Lisp Music}, maintained by the \emph{Center for Computer Research in Music and Acoustics} at Stanford University. CCRMA also maintains a variety of other projects, from DSL's like \emph{FAUST} and \emph{ChucK}, to \emph{STK}, a \emph{C}++ signal processing toolkit. All the above are open-source projects, and particularly good learning resources, but none is actually mainstream in terms of the size of the community that makes use of these tools. In recent years, an \emph{Objective-C} library named \emph{AudioKit} has gained popularity among macOS and iOS developers. \emph{AudioKit} manages high-level operations in \emph{Swift}, and is capable of running on \emph{Swift Playgrounds}, a platform that allows live coding and prototyping with great ease. What is particularly interesting about \emph{AudioKit} is that it remains open-source, and a lot of its underlying foundation relies of ports of \emph{Csound} routines.

\section{The Test of Time}

Currently, out of the multitude of tools and ideas devised between the Sixties and the early Nineties, the languages that really withstood the test of time were \emph{Csound} and \emph{Max}. Both eventually real-time \emph{synthesis} languages, but \emph{Max} became also a commercial application, whereas \emph{Csound} remains a community-maintained open source project. \emph{Max} was sold by the extinct Opcode Systems in the mid-Nineties, then was maintained and sold by Cycling '74 from the late nineties until the latter was sold to Ableton in the mid-2010's. \emph{Max} really blossomed in the 2000's, eventually becoming a digital arts platform, with vast support for image and video processing. The unit generator concept still applies: defining signal processing algorithms by dragging boxes around is easier, but more limited than creating the boxes themselves in \emph{C}. Over the years, \emph{Max} provided hooks to routines defined in other general-purpose programming languages, namely boxes that can execute \emph{Csound}, \emph{Java}, and more recently \emph{JavaScript} code, as well as \emph{Max}'s very own scripting language that is used inside \emph{GEN} boxes. This ability mitigates somehow some of the shortcomings of a very high-level language. Control statements in \emph{Max} are cumbersome and, although the concept of encapsulation exists in \emph{Max}, code re-usability is very much neglected. Still, \emph{Max} is easy enough to learn, very efficient and has surely survived to become an essential tool in most digital artists' arsenals.

\emph{Csound}, on the other hand, has experienced a steady-state phase since the late Nineties, with very little progress really made. Likely due to it being community-maintained, the language never really evolved much from its \emph{Music N} origins, and remains very limited. Open-source projects are notoriously difficult to change, as backward-compatibility is usually preferred over innovation. One can still compile and run \emph{Trapped in Convert} by Richard Boulanger, a piece written in \emph{Music 11} in 1979, then ported to \emph{Csound} in 1996, with the latest version of \emph{Csound}. At the same time, libraries are practically inexistent for the language, which has exhibited a tendency over the years to simply incorporate more and more unit generators to its compiler. This practice accounts for very poor documentation of the language in general, given how centralized its development is. It also accounts for a myriad of incoherent and outdated opcodes. User-defined opcodes, an alternative to extending the language in terms of itself, rather than in terms of its native \emph{C}, is unattractive and never really took. One of the biggest problems with \emph{Csound} remains that of algorithmically generating musical events. Although supported by the language to some extent, control statements have a very cumbersome syntax. Since the mid-2000's, the ability to execute \emph{Python} code from withing \emph{Csound} has improved somehow its ability to become a viable tool for algorithmic composition. One of the ways in which \emph{Csound} has grown over the years, however, is as an embeddable \emph{C}-library that is very fast and complete. Many of the shortcomings of its DSL suddenly become qualities when \emph{Csound} is embedded to a GUI application written in another language, such as its gigantic amount of opcodes.

Since the mid-Nineties, a third language has survived the test of time, namely \emph{Supercollider}. It is orders of magnitude more complete and versatile than \emph{Csound} and \emph{Max} combined, except for the latter's ability to render video, which \emph{Supercollider} lacks. It consists of three components: an interpreter, a DSL, and an IDE. The interpreter, which is also called synthesizer, may be used on its own and, like \emph{Csound}, is embeddable. All three components are multi-platform, open-source, and written in \emph{C++}. The DSL is both object-oriented and functional, and is very similar to \emph{Smalltalk}. In fact, \emph{Supercollider} has a huge library of classes that describe all sorts of signal processing algorithms, all of which written in terms of the language itself. The syntax of the DSL is its weakest point, and features symbols and conventions that are not easily readable by most computer programmers. Given that \emph{Supercollider} is too a unit-generator language, working with it consists mainly of combining ugens in interesting ways. As there are hundreds of ugens, mostly organized hierarchically as subclasses of one another, with many inheritance details, \emph{Supercollider} has a very steep learning curve, certainly the steepest of all music DSL's. That, combined with its \emph{Smalltalk}-like syntax, makes learning the language a very poor alternative to learning a general-purpose language. In fact, with adequate background, it may be easier to write a real-time audio implementation in a general-purpose language than to altogether learn \emph{Supercollider}. That said, since its creation in 1996 by James McCartney, the language has certainly endured and conquered its place in the Swiss-army knives of electro-acoustic music composers. Its IDE is the best-in-class, with access to documentation, code highlighting and completion, and is capable of executing code dynamically, line-by-line. This makes the DSL especially suited for live-coding applications and laptop ensembles.